# Environment variables template
# Copy this file to .env and fill in your values

# Required Configuration
APP_NAME=tech-article-generator
ENVIRONMENT=development

# Optional Configuration (defaults shown)
LOG_LEVEL=INFO
DEBUG=false
API_TIMEOUT=30

# Database Configuration (optional)
# DATABASE_URL=postgresql://user:password@localhost:5432/dbname
# DB_POOL_SIZE=5
# DB_MAX_OVERFLOW=10
# DB_POOL_TIMEOUT=30
# DB_MAX_RETRIES=3
# DB_RETRY_DELAY=1.0

# Secret Configuration (optional)
# API_KEY=your-api-key-here
# SECRET_KEY=your-secret-key-here

# LLM Configuration (Track 2)
# Hosted Providers (Gemini or OpenAI):
LLM_PROVIDER=gemini
LLM_API_KEY=your_gemini_api_key_here
LLM_DEFAULT_MODEL=gemini-1.5-flash
LLM_MAX_RETRIES=3
LLM_RETRY_DELAY=1.0
LLM_TIMEOUT=30

# Custom LLM Endpoint (Optional - for self-hosted/local LLMs)
# When CUSTOM_LLM_BASE_URL is set, it takes priority over LLM_PROVIDER
# Use this for Ollama, LM Studio, vLLM, or any OpenAI-compatible endpoint

# Example 1: Ollama (local)
# CUSTOM_LLM_BASE_URL=http://localhost:11434/v1
# CUSTOM_LLM_MODEL=llama2
# No API key needed for local Ollama

# Example 2: LM Studio (local)
# CUSTOM_LLM_BASE_URL=http://localhost:1234/v1
# CUSTOM_LLM_MODEL=local-model

# Example 3: vLLM Server (remote)
# CUSTOM_LLM_BASE_URL=http://your-server:8000/v1
# CUSTOM_LLM_API_KEY=your-api-key
# CUSTOM_LLM_MODEL=mistralai/Mistral-7B-Instruct-v0.2

# Example 4: Together AI
# CUSTOM_LLM_BASE_URL=https://api.together.xyz/v1
# CUSTOM_LLM_API_KEY=your-together-api-key
# CUSTOM_LLM_MODEL=mistralai/Mixtral-8x7B-Instruct-v0.1
